---
title: App
---

# App

```{replite}
import piplite
await piplite.install("ipywidgets")

:kernel: python
:height: 580px
import asyncio, json, re, os, html, glob, io, base64, traceback
from typing import List, Tuple, Dict, Optional

import ipywidgets as w
from IPython.display import display

# è©¦è‘—è¼‰å…¥å¯ç”¨å¥—ä»¶ï¼ˆPyodide ç’°å¢ƒé€šå¸¸æœ‰ pandasï¼Œä½† .ods éœ€è¦ odfpyï¼›è‹¥æ²’æœ‰å°±æ”¯æ´ .csvï¼‰
try:
    import pandas as pd
except Exception:
    pd = None

import xml.etree.ElementTree as ET

# =========================
# UI
# =========================
api_key = w.Password(description='API Key:', placeholder='sk-...')
base_url = w.Text(value='https://api.openai.com/v1', description='Base URL:')
model    = w.Text(value='gpt-4.1-mini', description='Model:')
batchsz  = w.IntSlider(value=16, min=1, max=64, step=1, description='Batch:')
limit_n  = w.IntText(value=50, description='è™•ç†ç­†æ•¸ä¸Šé™:')

ts_upl   = w.FileUpload(accept='.ts', multiple=False, description='ä¸Šå‚³ .ts')
gls_upl  = w.FileUpload(accept='.ods,.csv', multiple=False, description='glossary(é¸ç”¨)')

run_btn  = w.Button(description='Run', button_style='primary')
out      = w.Output()

display(
    w.VBox([
        w.HBox([api_key, base_url, model]),
        w.HBox([batchsz, limit_n]),
        w.HBox([ts_upl, gls_upl]),
        run_btn,
        out
    ])
)

# =========================
# å·¥å…·èˆ‡æµç¨‹ï¼ˆä¾ä½ çš„åŸå§‹é‚è¼¯ç²¾ç°¡èª¿æ•´ç‚ºç€è¦½å™¨å¯è·‘ï¼‰
# =========================
# é®ç½©/é‚„åŸï¼ˆHTML/placeholder/å¯¦é«”ä¿è­·ï¼‰
_MASK_PAT = re.compile(
    r'('
    r'</?[A-Za-z][^>]*>'
    r'|&lt;/?[A-Za-z][^&]*?&gt;'
    r'|%L\d+'
    r'|%\d+'
    r'|%n'
    r'|\{\d+\}'
    r'|&(?:[A-Za-z]+|#\d+|#x[0-9A-Fa-f]+);'
    r')',
    flags=re.IGNORECASE
)
def _mask_text(s: str) -> Tuple[str, Dict[str, str]]:
    idx = 0
    mapping: Dict[str, str] = {}
    def _repl(m):
        nonlocal idx
        key = f"âŸ¦MASK{idx}âŸ§"
        mapping[key] = m.group(0)
        idx += 1
        return key
    masked = _MASK_PAT.sub(_repl, s)
    return masked, mapping

def _unmask_text(s: str, mapping: Dict[str, str]) -> str:
    for k, v in mapping.items():
        s = s.replace(k, v)
    return s

def _et_ready(s: str) -> str:
    try:
        return html.unescape(s)
    except Exception:
        return s

def needs_translation(en_text: Optional[str]) -> bool:
    if not en_text or not en_text.strip():
        return False
    if re.fullmatch(r"[\s\d\W%{}]+", en_text):
        return False
    return True

# LCS/glossaryï¼ˆå¥å­å„ªå…ˆ + å–®å­—å‰ç¶´ï¼‰ï¼Œç”¨è¼ƒè¼•é‡å¯«æ³•
_SEP_RE = re.compile(r'[\s/_\-.]+')
def soft_norm(s: str) -> str:
    return _SEP_RE.sub(' ', s.lower()).strip()

class LCSMatcher:
    _TOKEN_RE = re.compile(r"[A-Za-z0-9]+(?:[\/_.-][A-Za-z0-9]+)*")
    def __init__(self, df):
        # df: éœ€æœ‰ 'en','zh'
        if df is None or df.empty:
            self.lookup = None
            self.soft_index = {}
            self.max_soft_len = 1
            return
        self.lookup = df.copy()
        self.lookup["en_soft"] = self.lookup["en"].map(soft_norm)
        self.soft_index = {}
        for _, row in self.lookup.iterrows():
            key = row["en_soft"]
            if key not in self.soft_index:
                self.soft_index[key] = (row["en"], row["zh"])
        self.max_soft_len = max((len(x.split()) for x in self.lookup["en_soft"]), default=1)

    def build_glossary_sentence_first(self, text: str, limit: int = 12) -> Dict[str, str]:
        if not self.lookup is not None:
            return {}
        text_clean = re.sub(_MASK_PAT, " ", text)
        tokens = self._TOKEN_RE.findall(text_clean)
        toks_lc = [t.lower() for t in tokens]
        n = len(toks_lc)
        covered = [False] * n
        glossary: Dict[str, str] = {}
        def _mark(i: int, j: int):
            for k in range(i, j): covered[k] = True
        win_max = min(n, getattr(self, "max_soft_len", n))
        for wlen in range(win_max, 0, -1):
            if len(glossary) >= limit: break
            for i in range(0, n - wlen + 1):
                if any(covered[k] for k in range(i, i + wlen)): continue
                phrase = " ".join(toks_lc[i:i + wlen])
                key = soft_norm(phrase)
                if key in self.soft_index:
                    en, zh = self.soft_index[key]
                    if en not in glossary:
                        glossary[en] = zh
                        _mark(i, i + wlen)
                        if len(glossary) >= limit: break
        return glossary

def _read_doctype(xml_text: str) -> str:
    m = re.search(r'<!DOCTYPE[^>]+>', xml_text)
    return m.group(0) if m else ""

def load_glossary_from_upload(upl: w.FileUpload):
    if (pd is None) or (upl is None) or (len(upl.value)==0):
        return None
    # åƒ…å–ç¬¬ä¸€å€‹æª”
    meta = list(upl.value.values())[0]
    name = meta['metadata']['name']
    content = meta['content']  # bytes
    bio = io.BytesIO(content)

    if name.lower().endswith('.csv'):
        try:
            df = pd.read_csv(bio)
            df = df.rename(columns={c: c.strip() for c in df.columns})
            if 'en' in df.columns and 'zh' in df.columns:
                df = df[['en','zh']].dropna().drop_duplicates(subset=['en'])
                return df
        except Exception as e:
            with out:
                print(f"[glossary] CSV è®€å–å¤±æ•—ï¼š{e}")
        return None

    if name.lower().endswith('.ods'):
        try:
            # éœ€è¦ odfpy å¼•æ“ï¼›è‹¥ç’°å¢ƒæ²’æœ‰æœƒä¸Ÿä¾‹å¤–
            df = pd.read_excel(bio, engine='odf')
            df = df.rename(columns={c: c.strip() for c in df.columns})
            if "è‹±æ–‡åç¨±" in df.columns and "ä¸­æ–‡åç¨±" in df.columns:
                sub = df[["è‹±æ–‡åç¨±", "ä¸­æ–‡åç¨±"]].copy()
                sub.columns = ["en", "zh"]
                sub = sub.dropna().drop_duplicates(subset=['en'])
                return sub
            elif 'en' in df.columns and 'zh' in df.columns:
                sub = df[['en','zh']].dropna().drop_duplicates(subset=['en'])
                return sub
            else:
                with out:
                    print("[glossary] ODS ç¼ºå°‘å¿…è¦æ¬„ä½ï¼ˆéœ€ã€Œè‹±æ–‡åç¨±ã€ã€Œä¸­æ–‡åç¨±ã€æˆ– en/zhï¼‰")
        except Exception as e:
            with out:
                print(f"[glossary] ODS è®€å–å¤±æ•—ï¼ˆå¯èƒ½ç¼º odfpyï¼‰ï¼š{e}")
        return None

    with out:
        print("[glossary] åƒ…æ”¯æ´ .ods æˆ– .csv")
    return None

async def call_chat_completions_batch(masked_texts: List[str], glossaries: List[Dict[str,str]]):
    """
    å–®æ¬¡å‘¼å«æ¨¡å‹ï¼Œè¦æ±‚å›å‚³ç­‰é•·çš„ JSON é™£åˆ—ï¼ˆå­—ä¸²ï¼‰ï¼Œèˆ‡ä½ çš„åŸç‰ˆé‚è¼¯ä¸€è‡´ã€‚
    """
    assert len(masked_texts) == len(glossaries)
    items = []
    for i, (t, g) in enumerate(zip(masked_texts, glossaries)):
        hints = [f"{en} -> {zh}" for en, zh in g.items()]
        items.append({"id": i, "text": t, "glossary": hints})

    system_prompt = (
        "ä½ æ˜¯å°ç£ GIS åœ¨åœ°åŒ–è­¯è€…ï¼Œå°‡å¤šå€‹ç¨ç«‹è‹±æ–‡ç‰‡æ®µç¿»ç‚ºè‡ªç„¶å°ˆæ¥­çš„ç¹é«”ä¸­æ–‡ï¼ˆå°ç£ï¼‰ã€‚\n"
        "å¿…å®ˆè¦å‰‡ï¼š\n"
        "â€¢ ä¿ç•™ä¸¦åŸæ¨£è¼¸å‡ºæ‰€æœ‰ âŸ¦MASKæ•¸å­—âŸ§ ç‰‡æ®µï¼›ä¸å¯å¢åˆªæˆ–æ”¹å‹•ã€‚\n"
        "â€¢ ä¸å¾—è¼¸å‡ºä»»ä½•è§£é‡‹ã€æ¨™é¡Œã€ç¨‹å¼ç¢¼æ¡†æˆ–å¤šé¤˜æ–‡å­—ã€‚\n"
        "â€¢ è«‹ã€åªè¼¸å‡ºã€ä¸€å€‹ JSON é™£åˆ—ï¼ˆå­—ä¸²é™£åˆ—ï¼‰ï¼Œé•·åº¦å¿…é ˆèˆ‡è¼¸å…¥ items ç›¸åŒï¼Œä¸”ä¾åŸé †åºå°æ‡‰ã€‚\n"
        "â€¢ Glossary åƒ…ä¾›åƒè€ƒï¼›è‹¥ä¸è‡ªç„¶å¯å¿½ç•¥ã€‚\n"
        "â€¢ ä¸è¦æ”¹å‹•ä»»ä½• HTML æ¨™ç±¤æˆ– HTML å¯¦é«”ã€‚"
    )

    user_prompt = (
        "è«‹é€ä¸€ç¿»è­¯ä¸‹åˆ— itemsã€‚æ¯å€‹ item æ ¼å¼å¦‚ä¸‹ï¼š\n"
        "{ \"id\": <åºè™Ÿ>, \"text\": \"<å«é®ç½©çš„è‹±æ–‡>\", \"glossary\": [\"en -> zh\", ...] }\n\n"
        "è«‹ã€åªè¼¸å‡ºã€ä¸€å€‹ JSON é™£åˆ—ï¼Œä¾‹å¦‚ï¼š\n"
        "[\"è­¯æ–‡0\", \"è­¯æ–‡1\", ...]\n\n"
        f"items = {json.dumps(items, ensure_ascii=False)}"
    )

    headers = {
        "Authorization": f"Bearer {api_key.value}",
        "Content-Type": "application/json",
    }
    body = {
        "model": model.value,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        "temperature": 0.2,
        # é ç•™è¼ƒå¤§ max_tokensï¼Œé¿å…é•·è¼¸å‡ºè¢«æˆªæ–·
        "max_tokens": max(512, 200*max(4, len(masked_texts)))
    }

    # ç”¨ pyodide çš„ pyfetchï¼ˆç€è¦½å™¨ fetchï¼‰ã€‚æ³¨æ„ï¼šå¯èƒ½å— CORS å½±éŸ¿ã€‚
    from pyodide.http import pyfetch
    resp = await pyfetch(
        f"{base_url.value.rstrip('/')}/chat/completions",
        method="POST",
        headers=headers,
        body=json.dumps(body),
    )
    data = await resp.json()
    if resp.status >= 400:
        raise RuntimeError(f"API Error {resp.status}: {data}")
    raw = (data["choices"][0]["message"]["content"] or "").strip()

    # åš´æ ¼è§£ææˆ list[str]
    def _parse_json_list(s: str):
        try:
            arr = json.loads(s)
            if isinstance(arr, list) and all(isinstance(x, str) for x in arr):
                return arr
        except Exception:
            pass
        lb = s.find('['); rb = s.rfind(']')
        if 0 <= lb < rb:
            chunk = s[lb:rb+1]
            arr = json.loads(chunk)
            if isinstance(arr, list) and all(isinstance(x, str) for x in arr):
                return arr
        raise ValueError("æ¨¡å‹è¼¸å‡ºéç´” JSON å­—ä¸²é™£åˆ—")
    return _parse_json_list(raw)

def build_download_link(filename: str, content_bytes: bytes) -> str:
    b64 = base64.b64encode(content_bytes).decode('utf-8')
    return f'<a download="{filename}" href="data:application/octet-stream;base64,{b64}">â¬‡ï¸ ä¸‹è¼‰ {filename}</a>'

async def run_pipeline(_):
    out.clear_output()
    try:
        if not api_key.value.strip():
            with out:
                print("è«‹å…ˆè¼¸å…¥ API Key")
            return
        if len(ts_upl.value) == 0:
            with out:
                print("è«‹å…ˆä¸Šå‚³ .ts æª”")
            return

        # è®€å– TS
        ts_meta = list(ts_upl.value.values())[0]
        ts_name = ts_meta['metadata']['name']
        ts_text = ts_meta['content'].decode('utf-8', errors='ignore')
        doctype = _read_doctype(ts_text)
        root = ET.fromstring(ts_text)
        messages = root.findall(".//message")

        # glossary
        df_gls = load_glossary_from_upload(gls_upl) if pd is not None else None
        matcher = LCSMatcher(df_gls)

        with out:
            print(f"ğŸ” å…± {len(messages)} å‰‡ <message>ï¼Œå°‡è™•ç†å‰ {limit_n.value} å‰‡ï¼ˆå¯èª¿æ•´ã€è™•ç†ç­†æ•¸ä¸Šé™ã€ï¼‰ã€‚")
            if df_gls is not None:
                print(f"ğŸ“˜ glossary æ¢ç›®æ•¸ï¼š{len(df_gls)}")
            else:
                print("ğŸ“˜ ç„¡ glossaryï¼ˆå¯ä¸Šå‚³ .csv æˆ– .odsï¼‰")

        # æ“·å–éœ€ç¿»è­¯ä»»å‹™
        tasks = []
        for m in messages:
            src_el = m.find("source")
            if src_el is None or src_el.text is None:
                continue
            if needs_translation(src_el.text):
                tasks.append((m, src_el.text, m.get("numerus") == "yes"))
            if len(tasks) >= limit_n.value:
                break

        finished = 0
        total = len(tasks)

        # æ‰¹æ¬¡è™•ç†
        for start in range(0, total, batchsz.value):
            batch = tasks[start:start+batchsz.value]
            glossaries, masked_texts, maps = [], [], []
            for _, src_text, _ in batch:
                g = matcher.build_glossary_sentence_first(src_text, limit=12) if matcher.lookup is not None else {}
                glossaries.append(g)
                masked, mp = _mask_text(src_text)
                masked_texts.append(masked)
                maps.append(mp)

            try:
                zh_list = await call_chat_completions_batch(masked_texts, glossaries)
            except Exception as e:
                with out:
                    print(f"[{start+1}..{start+len(batch)}] æ‰¹æ¬¡å¤±æ•—ï¼š{e}")
                # æ”¹é€ç­†ï¼ˆä¿å®ˆï¼‰
                zh_list = []
                for (m, src_text, _), masked, g in zip(batch, masked_texts, glossaries):
                    try:
                        one = await call_chat_completions_batch([masked],[g])
                        zh_list.append(one[0])
                    except Exception as ee:
                        with out:
                            print(f"  â”” é€ç­†å¤±æ•—ï¼Œå›åŸæ–‡ï¼š{ee}")
                        zh_list.append(src_text)

            for (m, src_text, is_num), zh_raw, mp in zip(batch, zh_list, maps):
                trans_el = m.find("translation")
                if trans_el is None:
                    trans_el = ET.SubElement(m, "translation")
                zh = _unmask_text(zh_raw, mp)
                zh = _et_ready(zh)
                if is_num:
                    forms = trans_el.findall("numerusform")
                    if not forms:
                        forms = [ET.SubElement(trans_el, "numerusform")]
                    for f in forms:
                        f.text = zh
                else:
                    trans_el.text = zh
                if "type" in trans_el.attrib:
                    trans_el.attrib.pop("type", None)
                finished += 1
                with out:
                    print(f"[{finished}/{total}] {repr(src_text[:60])} -> {repr(zh[:60])}")

        # å¯«å›ï¼ˆä¿ç•™å®£å‘Šèˆ‡ DOCTYPEï¼‰
        xml_bytes = ET.tostring(root, encoding="utf-8")
        header = b'<?xml version="1.0" encoding="utf-8"?>'
        if doctype:
            xml_bytes = header + ("\n"+doctype+"\n").encode("utf-8") + xml_bytes
        else:
            xml_bytes = header + b"\n" + xml_bytes

        # æª”å
        out_name = re.sub(r'\.ts$', '', ts_name) + "_zh-Hant.ts"
        link_html = build_download_link(out_name, xml_bytes)
        with out:
            from IPython.display import HTML
            display(HTML(f"<hr/>{link_html}"))
            print("âœ… å®Œæˆï¼ˆä¸Šé¢é€£çµå¯ä¸‹è¼‰çµæœï¼‰")

    except Exception as e:
        with out:
            print("ç™¼ç”ŸéŒ¯èª¤ï¼š", e)
            traceback.print_exc()

run_btn.on_click(lambda _: asyncio.ensure_future(run_pipeline(_)))
```
