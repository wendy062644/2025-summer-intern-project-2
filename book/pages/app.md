
<hr/>

### `translate_app.md`
```markdown
# TS Translator App

> æŒ‰ä¸Šæ–¹çš„ **âš¡ï¸ Live Code** å•Ÿå‹•äº’å‹•æ¨¡å¼ï¼ˆThebeï¼‰ã€‚

<hr/>

## 1) åŸå§‹ç¢¼ï¼ˆç¿»è­¯å¼•æ“ï¼‰

```{code-cell} ipython3
:tags: [hide-output]

# -*- coding: utf-8 -*-
"""
Qt .ts ç¿»è­¯ï¼ˆOpenAI ChatGPT API + LCS è©å½™æç¤º + HTML/ä½”ä½ç¬¦é®ç½©ä¿ç•™ï¼‰
å¼·åŒ–ç‰ˆï¼šå»é‡ã€ä½µç™¼å¤šæ‰¹ã€æ‰¹æ¬¡ JSON è¼¸å‡ºã€é€²åº¦æ¢ï¼ˆæ”¯æ´ widgets å›å‘¼ï¼‰
"""
import os, re, glob, html, json, time, random, shutil
from typing import List, Tuple, Dict, Optional, Callable
from concurrent.futures import ThreadPoolExecutor, as_completed

import pandas as pd
import xml.etree.ElementTree as ET

try:
    from tqdm.auto import tqdm  # CLI fallback
except Exception:
    class _NoTQDM:
        def __init__(self, total=None, desc=None, unit=None): pass
        def update(self, n=1): pass
        def close(self): pass
    def tqdm(*args, **kwargs):
        return _NoTQDM()

from openai import OpenAI

# ================== å·¥å…·ï¼šè®€ DOCTYPE ==================
def _read_doctype(xml_text: str) -> str:
    m = re.search(r'<!DOCTYPE[^>]+>', xml_text)
    return m.group(0) if m else ""

# ================== è®€ ODS è©åº«ï¼ˆè‹¥ç„¡å‰‡ç©ºè¡¨ï¼‰ ==================
def load_lookup_from_ods(folder: str = "data") -> pd.DataFrame:
    paths = sorted(glob.glob(os.path.join(folder, "*.ods")))
    if not paths:
        # ç·šä¸Šç’°å¢ƒå¸¸ç„¡ ODSï¼Œå›å‚³ç©ºè¡¨å³å¯
        print(f"[æç¤º] è©åº«è³‡æ–™å¤¾ {folder} ç„¡ .odsï¼›å°‡åœ¨ç„¡ glossary æ¨¡å¼ä¸‹é‹ä½œã€‚")
        return pd.DataFrame({"en": [], "zh": []})
    rows = []
    for p in paths:
        try:
            df = pd.read_excel(p, engine="odf")
            if "è‹±æ–‡åç¨±" in df.columns and "ä¸­æ–‡åç¨±" in df.columns:
                sub = df[["è‹±æ–‡åç¨±", "ä¸­æ–‡åç¨±"]].copy()
                sub.columns = ["en", "zh"]
                rows.append(sub)
            else:
                print(f"[ç•¥é] {p} ç¼ºå°‘ã€è‹±æ–‡åç¨±/ä¸­æ–‡åç¨±ã€æ¬„ä½")
        except Exception as e:
            print(f"[è­¦å‘Š] ç„¡æ³•è®€å– {p}: {e}")
    if not rows:
        print("[æç¤º] æ‰¾ä¸åˆ°å¯ç”¨çš„è©åº«æ¬„ä½ï¼Œæ”¹ç‚ºç©ºè¡¨ã€‚")
        return pd.DataFrame({"en": [], "zh": []})

    tbl = pd.concat(rows, ignore_index=True)
    tbl["en"] = tbl["en"].astype(str).str.strip()
    tbl["zh"] = tbl["zh"].astype(str).str.strip()
    tbl = tbl.dropna(subset=["en", "zh"])
    tbl = tbl[(tbl["en"] != "") & (tbl["zh"] != "")]
    tbl = tbl.drop_duplicates(subset=["en"], keep="first").reset_index(drop=True)
    return tbl

# ================== LCS æ¯”å°å·¥å…· ==================
def normalize_token(s: str) -> str: return s.lower()
def normalize_cand(s: str) -> str:  return s.lower()

def anchored_prefix_sub_in(token_norm: str, cand_norm: str) -> Tuple[int, str]:
    if not token_norm or not cand_norm:
        return 0, ""
    max_k = min(len(token_norm), len(cand_norm))
    for k in range(max_k, 0, -1):
        sub = token_norm[:k]
        if sub in cand_norm:
            return k, sub
    return 0, ""

_SEP_RE = re.compile(r'[\s/_\-.]+')
def soft_norm(s: str) -> str:
    return _SEP_RE.sub(' ', s.lower()).strip()

_GLOSSARY_FILTER_PAT = re.compile(
    r'('
    r'</?[A-Za-z][^>]*>'
    r'|&lt;/?[A-Za-z][^&]*?&gt;'
    r'|%L\d+'
    r'|%\d+'
    r'|%n'
    r'|\{\d+\}'
    r'|&(?:[A-Za-z]+|#\d+|#x[0-9A-Fa-f]+);'
    r')',
    flags=re.IGNORECASE
)
def _clean_for_glossary(text: str) -> str:
    return _GLOSSARY_FILTER_PAT.sub(' ', text)

class LCSMatcher:
    _TOKEN_RE = re.compile(r"[A-Za-z0-9]+(?:[\/_.-][A-Za-z0-9]+)*")

    def __init__(self, lookup_df: pd.DataFrame, min_token_len: int = 4, min_lcs_len: int = 4):
        self.lookup = lookup_df.copy()
        self.lookup["en_norm"] = self.lookup["en"].apply(normalize_cand) if len(self.lookup) else []
        if len(self.lookup):
            self.lookup = self.lookup[self.lookup["en_norm"].str.len() >= 1].reset_index(drop=True)
            self.lookup["charset"] = self.lookup["en_norm"].apply(lambda s: set(re.sub(r"\s+", "", s)))
            self.lookup["en_soft"] = self.lookup["en"].apply(soft_norm)
            self.soft_index: Dict[str, Tuple[str, str]] = {}
            for _, row in self.lookup.iterrows():
                key = row["en_soft"]
                if key not in self.soft_index:
                    self.soft_index[key] = (row["en"], row["zh"])
            self.max_soft_len = max((len(x.split()) for x in self.lookup["en_soft"]), default=1)
        else:
            self.lookup = pd.DataFrame({"en": [], "zh": [], "en_norm": []})
            self.soft_index = {}
            self.max_soft_len = 1
        self.min_token_len = min_token_len
        self.min_lcs_len = min_lcs_len

    def _topk_for_word(self, token: str, k: int = 3) -> List[Dict]:
        if len(self.lookup) == 0:
            return []
        t_norm = normalize_token(token)
        if len(t_norm) < self.min_token_len:
            return []
        t_chars = set(t_norm)
        candidates = self.lookup[self.lookup["charset"].apply(lambda s: len(s & t_chars) > 0)]
        results = []
        for _, row in candidates.iterrows():
            kk, sub = anchored_prefix_sub_in(t_norm, row["en_norm"])
            if kk >= self.min_lcs_len:
                results.append({
                    "token": token, "en": row["en"], "zh": row["zh"],
                    "lcs_len": kk, "lcs": sub
                })
        results.sort(key=lambda d: (-d["lcs_len"], len(d["en"])))
        return results[:k]

    def build_glossary_sentence_first(
        self, text: str, *, limit: int = 8, per_word_k: int = 2, min_lcs_len: int = 4,
    ) -> Dict[str, str]:
        if len(self.lookup) == 0:
            return {}
        text_clean = _clean_for_glossary(text)
        tokens = self._TOKEN_RE.findall(text_clean)
        toks_lc = [t.lower() for t in tokens]
        n = len(toks_lc)
        covered = [False] * n
        glossary: Dict[str, str] = {}

        def _mark(i: int, j: int):
            for k in range(i, j): covered[k] = True

        win_max = min(n, getattr(self, "max_soft_len", n))
        for w in range(win_max, 0, -1):
            if len(glossary) >= limit: break
            for i in range(0, n - w + 1):
                if any(covered[k] for k in range(i, i + w)): continue
                phrase = " ".join(toks_lc[i:i + w])
                key = soft_norm(phrase)
                if key in self.soft_index:
                    en, zh = self.soft_index[key]
                    if en not in glossary:
                        glossary[en] = zh
                        _mark(i, i + w)
                        if len(glossary) >= limit: break

        for idx, tok in enumerate(tokens):
            if len(glossary) >= limit: break
            if covered[idx]: continue
            if len(tok) < min_lcs_len: continue
            hits = self._topk_for_word(tok, k=per_word_k)
            for r in hits:
                if r["lcs_len"] >= min_lcs_len and r["en"] not in glossary:
                    glossary[r["en"]] = r["zh"]
                    covered[idx] = True
                    if len(glossary) >= limit: break
        return glossary

# ================== é®ç½©/é‚„åŸ ==================
_MASK_PAT = re.compile(
    r'('
    r'</?[A-Za-z][^>]*>'
    r'|&lt;/?[A-Za-z][^&]*?&gt;'
    r'|%L\d+'
    r'|%\d+'
    r'|%n'
    r'|\{\d+\}'
    r'|&(?:[A-Za-z]+|#\d+|#x[0-9A-Fa-f]+);'
    r')',
    flags=re.IGNORECASE
)

def _mask_text(s: str) -> Tuple[str, Dict[str, str]]:
    idx = 0
    mapping: Dict[str, str] = {}
    def _repl(m):
        nonlocal idx
        key = f"âŸ¦MASK{idx}âŸ§"
        mapping[key] = m.group(0)
        idx += 1
        return key
    masked = _MASK_PAT.sub(_repl, s)
    return masked, mapping

def _unmask_text(s: str, mapping: Dict[str, str]) -> str:
    for k, v in mapping.items():
        s = s.replace(k, v)
    return s

def _et_ready(s: str) -> str:
    try:
        return html.unescape(s)
    except Exception:
        return s

# ================== åˆ¤æ–·æ˜¯å¦éœ€è¦ç¿»è­¯ ==================
def needs_translation(en_text: Optional[str]) -> bool:
    if not en_text or not en_text.strip():
        return False
    if re.fullmatch(r"[\s\d\W%{}]+", en_text):
        return False
    return True

# ================== OpenAI å‘¼å«ï¼ˆå«é‡è©¦ï¼‰ ==================
def _with_retry(fn, *, tries=4, base=0.6, cap=4.0):
    last = None
    for t in range(tries):
        try:
            return fn()
        except Exception as e:
            last = e
            if t == tries - 1:
                break
            sleep = min(cap, base * (2 ** t)) * (0.8 + 0.4 * random.random())
            time.sleep(sleep)
    raise last if last else RuntimeError("æœªçŸ¥éŒ¯èª¤")

def chatgpt_translate(masked_text: str, glossary: Dict[str, str], *, model: str, client: OpenAI) -> str:
    glossary_str = "\n".join([f"- {en} -> {zh}" for en, zh in glossary.items()]) or "ï¼ˆç„¡ï¼‰"
    system_prompt = (
        "ä½ æ˜¯å°ç£ GIS è»Ÿé«”åœ¨åœ°åŒ–è­¯è€…ï¼Œè«‹å°‡è‹±æ–‡ç¿»ç‚ºè‡ªç„¶å°ˆæ¥­çš„ç¹é«”ä¸­æ–‡ï¼ˆå°ç£ï¼‰ã€‚\n"
        "åš´æ ¼è¦å‰‡ï¼š\n"
        "1) âŸ¦MASKiâŸ§ åŸæ¨£ä¿ç•™ä¸”ä¸å¯å¢åˆªï¼›\n"
        "2) ä¸å¾—è¼¸å‡ºä»»ä½•è§£é‡‹æˆ–å‰å¾Œç¶´ï¼›åªè¼¸å‡ºè­¯æ–‡ï¼›\n"
        "3) Glossary åƒ…ä¾›åƒè€ƒï¼Œè‹¥ä¸è‡ªç„¶å¯å¿½ç•¥ï¼›\n"
        "4) ä¸è¦æ”¹å‹•ä»»ä½• HTML æ¨™ç±¤æˆ– HTML å¯¦é«”ï¼›"
    )
    user_prompt = (
        f"è©å½™å°ç…§ï¼ˆåƒ…ä¾›åƒè€ƒï¼‰ï¼š\n{glossary_str}\n\n"
        f"è‹±æ–‡å¥å­ï¼ˆå«é®ç½©ï¼‰ï¼š\n{masked_text}\n\n"
        "è«‹ç›´æ¥è¼¸å‡ºæœ€çµ‚ä¸­æ–‡è­¯æ–‡ï¼š"
    )
    def _call():
        resp = client.chat.completions.create(
            model=model,
            messages=[{"role": "system", "content": system_prompt},
                      {"role": "user", "content": user_prompt}],
            temperature=0.2,
            max_tokens=100,
        )
        return (resp.choices[0].message.content or "").strip()
    zh = _with_retry(_call)
    for line in zh.splitlines():
        s = line.strip().strip("ã€Œã€\"'")
        if s:
            return s
    return ""

def chatgpt_translate_batch(masked_texts: List[str], glossaries: List[Dict[str, str]], *, model: str, client: OpenAI) -> List[str]:
    assert len(masked_texts) == len(glossaries), "masked_texts / glossaries é•·åº¦ä¸ä¸€è‡´"

    items = []
    for i, (t, g) in enumerate(zip(masked_texts, glossaries)):
        hints = [f"{en} -> {zh}" for en, zh in g.items()]
        items.append({"id": i, "text": t, "glossary": hints})

    system_prompt = (
        "ä½ æ˜¯å°ç£ GIS åœ¨åœ°åŒ–è­¯è€…ï¼Œå°‡å¤šå€‹ç¨ç«‹è‹±æ–‡ç‰‡æ®µç¿»ç‚ºè‡ªç„¶å°ˆæ¥­çš„ç¹é«”ä¸­æ–‡ï¼ˆå°ç£ï¼‰ã€‚\n"
        "å¿…å®ˆè¦å‰‡ï¼š\n"
        "â€¢ ä¿ç•™ä¸¦åŸæ¨£è¼¸å‡ºæ‰€æœ‰ âŸ¦MASKæ•¸å­—âŸ§ ç‰‡æ®µï¼›ä¸å¯å¢åˆªæˆ–æ”¹å‹•ã€‚\n"
        "â€¢ ä¸å¾—è¼¸å‡ºä»»ä½•è§£é‡‹ã€æ¨™é¡Œã€ç¨‹å¼ç¢¼æ¡†æˆ–å¤šé¤˜æ–‡å­—ã€‚\n"
        "â€¢ è«‹ã€åªè¼¸å‡ºã€ä¸€å€‹ JSON é™£åˆ—ï¼ˆå­—ä¸²é™£åˆ—ï¼‰ï¼Œé•·åº¦å¿…é ˆèˆ‡è¼¸å…¥ items ç›¸åŒï¼Œä¸”ä¾åŸé †åºå°æ‡‰ã€‚\n"
        "â€¢ Glossary åƒ…ä¾›åƒè€ƒï¼›è‹¥ä¸è‡ªç„¶å¯å¿½ç•¥ã€‚\n"
        "â€¢ ä¸è¦æ”¹å‹•ä»»ä½• HTML æ¨™ç±¤æˆ– HTML å¯¦é«”ã€‚"
    )

    user_prompt = (
        "è«‹é€ä¸€ç¿»è­¯ä¸‹åˆ— itemsã€‚æ¯å€‹ item æ ¼å¼å¦‚ä¸‹ï¼š\n"
        "{ \"id\": <åºè™Ÿ>, \"text\": \"<å«é®ç½©çš„è‹±æ–‡>\", \"glossary\": [\"en -> zh\", ...] }\n\n"
        "è«‹ã€åªè¼¸å‡ºã€ä¸€å€‹ JSON é™£åˆ—ï¼Œä¾‹å¦‚ï¼š\n"
        "[\"è­¯æ–‡0\", \"è­¯æ–‡1\", ...]\n\n"
        f"items = {json.dumps(items, ensure_ascii=False)}"
    )

    def _call():
        resp = client.chat.completions.create(
            model=model,
            messages=[{"role": "system", "content": system_prompt},
                      {"role": "user", "content": user_prompt}],
            temperature=0.2,
            max_tokens=min(4096, 160 * max(4, len(masked_texts))),
        )
        return (resp.choices[0].message.content or "").strip()

    raw = _with_retry(_call)

    def _parse_json_list(s: str) -> List[str]:
        import json as _json
        try:
            arr = _json.loads(s)
            if isinstance(arr, list) and all(isinstance(x, str) for x in arr):
                return arr
        except Exception:
            pass
        lb = s.find('[')
        rb = s.rfind(']')
        if 0 <= lb < rb:
            chunk = s[lb:rb+1]
            arr = _json.loads(chunk)
            if isinstance(arr, list) and all(isinstance(x, str) for x in arr):
                return arr
        raise ValueError("æ¨¡å‹è¼¸å‡ºéç´” JSON å­—ä¸²é™£åˆ—")

    arr = _parse_json_list(raw)
    if len(arr) != len(masked_texts):
        raise ValueError(f"JSON é™£åˆ—é•·åº¦ä¸ç¬¦ï¼ŒæœŸå¾… {len(masked_texts)}ï¼Œå¾—åˆ° {len(arr)}")
    return arr

# ================== ç¿»è­¯è¼”åŠ© ==================
def translate_one(src_text: str, matcher: LCSMatcher, *, model: str, client: OpenAI) -> str:
    glossary = matcher.build_glossary_sentence_first(src_text, limit=8, per_word_k=2, min_lcs_len=4)
    masked, mapping = _mask_text(src_text)
    zh_raw = chatgpt_translate(masked, glossary, model=model, client=client)
    if not zh_raw:
        return src_text
    zh = _unmask_text(zh_raw, mapping)
    if src_text.endswith("\n") and not zh.endswith("\n"):
        zh += "\n"
    return zh

def translate_chunk(chunk_srcs: List[str], matcher: LCSMatcher, *, model: str, client: OpenAI) -> List[str]:
    glossaries, masked_texts, mappings = [], [], []
    for s in chunk_srcs:
        g = matcher.build_glossary_sentence_first(s, limit=8, per_word_k=2, min_lcs_len=4)
        glossaries.append(g)
        masked, mp = _mask_text(s)
        masked_texts.append(masked)
        mappings.append(mp)
    try:
        zh_list = chatgpt_translate_batch(masked_texts, glossaries, model=model, client=client)
    except Exception:
        zh_list = []
        for s in chunk_srcs:
            try:
                zh_list.append(translate_one(s, matcher, model=model, client=client))
            except Exception:
                zh_list.append(s)
    return [_et_ready(_unmask_text(z, mp)) for z, mp in zip(zh_list, mappings)]

# ================== ä¸»æµç¨‹ï¼ˆæ”¯æ´ widgets é€²åº¦ï¼‰ ==================
def process_ts_openai(input_ts: str, output_ts: str, matcher: LCSMatcher, *, 
                      openai_api_key: str, openai_model: str = "gpt-4.1-mini",
                      batch_size: int = 16, concurrency: int = 4,
                      on_total: Optional[Callable[[int], None]] = None,
                      on_progress: Optional[Callable[[int], None]] = None,
                      on_log: Optional[Callable[[str], None]] = None):
    """
    on_total(total)   ï¼šå›å ± unique åŸæ–‡æ•¸
    on_progress(delta)ï¼šæ¯å®Œæˆ delta ç­† unique åŸæ–‡æ™‚å‘¼å«
    on_log(msg)       ï¼šå³æ™‚è¨Šæ¯è¼¸å‡º
    """
    if on_log: on_log(f"é–‹å§‹è™•ç†ï¼š{input_ts}")
    client = OpenAI(api_key=openai_api_key)
    # å‚™ä»½
    shutil.copyfile(input_ts, f"{input_ts}.bak")
    raw_xml = open(input_ts, "r", encoding="utf-8").read()
    doctype = _read_doctype(raw_xml)

    tree = ET.ElementTree(ET.fromstring(raw_xml))
    root = tree.getroot()

    messages = root.findall(".//message")
    tasks = []
    for m in messages:
        src_el = m.find("source")
        if src_el is None or src_el.text is None:
            continue
        if needs_translation(src_el.text):
            tasks.append((m, src_el.text, m.get("numerus") == "yes"))

    total_msg = len(messages)
    need_cnt = len(tasks)
    if on_log: on_log(f"ğŸ” åŸå§‹æª”å…± {total_msg} ç­† messageï¼Œéœ€ç¿»è­¯ {need_cnt} ç­†")

    if not tasks:
        xml_bytes = ET.tostring(root, encoding="utf-8")
        with open(output_ts, "wb") as f:
            f.write(b'<?xml version="1.0" encoding="utf-8"?>')
            if doctype:
                f.write(("\n" + doctype + "\n").encode("utf-8"))
            f.write(xml_bytes)
        if on_log: on_log(f"âœ… ç„¡éœ€ç¿»è­¯ï¼Œè¼¸å‡º -> {output_ts}")
        return

    # å»é‡
    idx_map: Dict[str, int] = {}
    uniq_srcs: List[str] = []
    task_uid: List[int] = []
    for _m, src, _num in tasks:
        if src not in idx_map:
            idx_map[src] = len(uniq_srcs)
            uniq_srcs.append(src)
        task_uid.append(idx_map[src])

    if on_total: on_total(len(uniq_srcs))

    uniq_zh = [""] * len(uniq_srcs)
    chunks = [uniq_srcs[i:i+batch_size] for i in range(0, len(uniq_srcs), batch_size)]

    pbar = tqdm(total=len(uniq_srcs), desc="ç¿»è­¯å”¯ä¸€å­—ä¸²", unit="ç­†")
    with ThreadPoolExecutor(max_workers=concurrency) as ex:
        futures = {ex.submit(translate_chunk, ch, matcher, model=openai_model, client=client): (idx, ch) 
                   for idx, ch in enumerate(chunks)}
        for fut in as_completed(futures):
            idx, ch = futures[fut]
            base = idx * batch_size
            try:
                result = fut.result()
            except Exception as e:
                if on_log: on_log(f"[è­¦å‘Š] æ‰¹æ¬¡å¤±æ•—ï¼ˆé™ç´šé€ç­†ï¼‰ï¼š{e}")
                result = [translate_one(s, matcher, model=openai_model, client=client) for s in ch]
            for j, zh in enumerate(result):
                if base + j < len(uniq_zh):
                    uniq_zh[base + j] = zh
            pbar.update(len(ch))
            if on_progress: on_progress(len(ch))
    pbar.close()

    # å›å¡«
    for (m, _src_text, is_numerus), uid in zip(tasks, task_uid):
        trans_el = m.find("translation")
        if trans_el is None:
            trans_el = ET.SubElement(m, "translation")
        zh = uniq_zh[uid] or _src_text
        if is_numerus:
            forms = trans_el.findall("numerusform")
            if not forms:
                forms = [ET.SubElement(trans_el, "numerusform")]
            for f in forms:
                f.text = zh
        else:
            trans_el.text = zh
        trans_el.attrib.pop("type", None)

    xml_bytes = ET.tostring(root, encoding="utf-8")
    with open(output_ts, "wb") as f:
        f.write(b'<?xml version="1.0" encoding="utf-8"?>')
        if doctype:
            f.write(("\n" + doctype + "\n").encode("utf-8"))
        f.write(xml_bytes)

    if on_log: on_log(f"âœ… å®Œæˆç¿»è­¯ {len(tasks)} ç­†ï¼ˆå”¯ä¸€ {len(uniq_srcs)} ç­†ï¼‰ï¼Œè¼¸å‡º -> {output_ts}")

import io, os, pathlib
import ipywidgets as W
from IPython.display import display, FileLink

# --- Widgets ---
api_box   = W.Password(description="API Key", placeholder="sk-...")
model_box = W.Text(value="gpt-4.1-mini", description="Model")
batch_box = W.BoundedIntText(value=16, min=1, max=64, step=1, description="Batch")
conc_box  = W.BoundedIntText(value=4, min=1, max=16, step=1, description="Concurrency")

upload_ts = W.FileUpload(accept=".ts", multiple=False, description="ä¸Šå‚³ .ts")
start_btn = W.Button(description="é–‹å§‹", button_style="success")
progress  = W.IntProgress(value=0, min=0, max=1, description="é€²åº¦")
log_out   = W.Output(layout={"border":"1px solid #ddd"})

controls = W.VBox([api_box, model_box, W.HBox([batch_box, conc_box]), upload_ts, start_btn, progress, log_out])
display(controls)

workdir = pathlib.Path("./work")
workdir.mkdir(exist_ok=True)

def _log(msg):
    with log_out:
        print(msg)

def _on_total(total):
    progress.max = max(1, int(total))
    progress.value = 0
    progress.description = f"é€²åº¦ (å…± {total})"

def _on_progress(delta):
    progress.value = min(progress.max, progress.value + int(delta))

def _save_upload(uploader: W.FileUpload, dst_path: pathlib.Path) -> bool:
    if not uploader.value:
        _log("è«‹å…ˆä¸Šå‚³ .ts æª”")
        return False
    item = list(uploader.value.values())[0]
    data = item["content"]
    dst_path.write_bytes(data)
    _log(f"å·²å„²å­˜ï¼š{dst_path}")
    return True

def _on_start(_btn):
    log_out.clear_output()
    if not api_box.value.strip():
        _log("è«‹è¼¸å…¥ OpenAI API Key")
        return
    src = workdir / "input.ts"
    out = workdir / "output_zh-Hant.ts"
    if not _save_upload(upload_ts, src):
        return
    try:
        df_lookup = load_lookup_from_ods("data")  # è‹¥ç„¡ ODS ä¹Ÿæœƒæ­£å¸¸å›å‚³ç©ºè¡¨
        matcher = LCSMatcher(df_lookup, min_token_len=4, min_lcs_len=4)
        process_ts_openai(
            str(src), str(out), matcher,
            openai_api_key=api_box.value.strip(),
            openai_model=model_box.value.strip() or "gpt-4.1-mini",
            batch_size=int(batch_box.value),
            concurrency=int(conc_box.value),
            on_total=_on_total,
            on_progress=_on_progress,
            on_log=_log,
        )
        _log("å®Œæˆã€‚")
        display(FileLink(out, result_html_prefix="ä¸‹è¼‰çµæœï¼š"))
    except Exception as e:
        _log(f"[éŒ¯èª¤] {e}")

start_btn.on_click(_on_start)