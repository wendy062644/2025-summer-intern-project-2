{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2beadb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 升級 pip\n",
    "!pip install -U pip\n",
    "\n",
    "# 安裝套件\n",
    "!pip install -U pandas lxml tqdm transformers huggingface_hub tokenizers sentencepiece safetensors odfpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8da722-c8b9-4ada-b0f7-2930aef7044c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The repository THUDM/glm-4-9b-chat contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/THUDM/glm-4-9b-chat .\n",
      " You can inspect the repository content at https://hf.co/THUDM/glm-4-9b-chat.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n",
      "The repository THUDM/glm-4-9b-chat contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/THUDM/glm-4-9b-chat .\n",
      " You can inspect the repository content at https://hf.co/THUDM/glm-4-9b-chat.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-26 07:55:30.685481: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756194930.707929     234 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756194930.715253     234 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1756194930.733861     234 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756194930.733885     234 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756194930.733887     234 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756194930.733889     234 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The repository THUDM/glm-4-9b-chat contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/THUDM/glm-4-9b-chat .\n",
      " You can inspect the repository content at https://hf.co/THUDM/glm-4-9b-chat.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f17747f612f461191dd0cacf78b1c46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU count: 1\n",
      "Current GPU: NVIDIA GeForce RTX 3090\n",
      "Model repo: THUDM/glm-4-9b-chat\n",
      "Model dtype: torch.bfloat16\n",
      "Device map keys (sample): ['transformer']\n",
      "[TS] 待翻譯節點：2711（translation 與 numerusform 皆以 <source> 為翻譯來源）\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch translating:   0%|          | 0/170 [00:00<?, ?it/s]\n",
      "=== [PROMPT 1/2711] (numerusform) ===\n",
      "SOURCE: Showing {0} - {1} of %n result(s)\n",
      "HINTS: \n",
      "\n",
      "=== [PROMPT 2/2711] (numerusform) ===\n",
      "SOURCE: Showing {0} - {1} of %n result(s)\n",
      "HINTS: \n",
      "\n",
      "=== [PROMPT 3/2711] (translation) ===\n",
      "SOURCE: <html><body><h2>Algorithm description</h2>\n",
      "HINTS: algorithm -> 演算法\n",
      "\n",
      "=== [PROMPT 4/2711] (translation) ===\n",
      "SOURCE: Active layer is not a vector layer.\n",
      "HINTS: vector -> 向量\n",
      "\n",
      "=== [PROMPT 5/2711] (translation) ===\n",
      "SOURCE: Active layer is not editable (and editing could not be turned on).\n",
      "HINTS: \n",
      "\n",
      "=== [PROMPT 6/2711] (translation) ===\n",
      "SOURCE: Selected algorithm and parameter configuration are not compatible with in-place modifications.\n",
      "HINTS: algorithm -> 演算法 | parameter -> 參數\n",
      "\n",
      "=== [PROMPT 7/2711] (translation) ===\n",
      "SOURCE: Could not prepare selected algorithm.\n",
      "HINTS: algorithm -> 演算法\n",
      "\n",
      "=== [PROMPT 8/2711] (translation) ===\n",
      "SOURCE: Error adding processed features back into the layer.\n",
      "HINTS: \n",
      "\n",
      "=== [PROMPT 9/2711] (translation) ===\n",
      "SOURCE: <h2>Input parameters</h2>\n",
      "HINTS: input -> 輸入\n",
      "\n",
      "=== [PROMPT 10/2711] (translation) ===\n",
      "SOURCE: <h2>Outputs</h2>\n",
      "HINTS: \n",
      "\n",
      "=== [PROMPT 11/2711] (translation) ===\n",
      "SOURCE: <p align=\"right\">Algorithm author: {0}</p>\n",
      "HINTS: algorithm -> 演算法\n",
      "\n",
      "=== [PROMPT 12/2711] (translation) ===\n",
      "SOURCE: <p align=\"right\">Help author: {0}</p>\n",
      "HINTS: \n",
      "\n",
      "=== [PROMPT 13/2711] (translation) ===\n",
      "SOURCE: <p align=\"right\">Algorithm version: {0}</p>\n",
      "HINTS: algorithm -> 演算法 | version -> 版本\n",
      "\n",
      "=== [PROMPT 14/2711] (translation) ===\n",
      "SOURCE: Error\n",
      "HINTS: \n",
      "\n",
      "=== [PROMPT 15/2711] (translation) ===\n",
      "SOURCE: Seems some outputs are temporary files. To create test you need to redirect all algorithm outputs to files\n",
      "HINTS: algorithm -> 演算法\n",
      "\n",
      "=== [PROMPT 16/2711] (translation) ===\n",
      "SOURCE: There is no active layer.\n",
      "HINTS: \n",
      "Showing {0} - {1} of %n result(s) -> 顯示  - 共  結果{0}{1}%n\n",
      "Showing {0} - {1} of %n result(s) -> 顯示  - 共  結果{0}{1}%n\n",
      "Algorithm description -> 演算法描述\n",
      "Active layer is not a vector layer. -> 活動圖層不是向量圖層。\n",
      "Active layer is not editable (and editing could not be turned on). -> 活動層不可編輯（且無法啟用編輯）\n",
      "Selected algorithm and parameter configuration are not compatible with in-place modifications. -> 選取的演算法和參數配置不兼容內置修改。\n",
      "Could not prepare selected algorithm. -> 無法準備選擇的演算法。\n",
      "Error adding processed features back into the layer. -> 錯誤加入處理後的特徵回圖層\n",
      "Input parameters -> 輸入參數\n",
      "Outputs -> 輸出\n",
      "Algorithm author: {0} -> 演算法作者: {0}\n",
      "Help author: {0} -> 輔助作者: {0}\n",
      "Algorithm version: {0} -> 演算法版本: {0}\n",
      "Error -> 錯誤\n",
      "Seems some outputs are temporary files. To create test you need to redirect all algorithm outputs to files -> 看起來有些輸出是暫時文件。要建立測試，您需要將所有演算法輸出重定向到文件\n",
      "There is no active layer. -> 沒有活躍圖層。\n",
      "Batch translating:   1%|          | 1/170 [00:36<1:41:29, 36.03s/it]\n",
      "=== [PROMPT 17/2711] (translation) ===\n",
      "SOURCE: API Request / Response\n",
      "HINTS: request -> 要求 | response -> 回應\n",
      "\n",
      "=== [PROMPT 18/2711] (translation) ===\n",
      "SOURCE: Request\n",
      "HINTS: request -> 要求\n",
      "\n",
      "=== [PROMPT 19/2711] (translation) ===\n",
      "SOURCE: Response\n",
      "HINTS: response -> 回應\n",
      "\n",
      "=== [PROMPT 20/2711] (translation) ===\n",
      "SOURCE: Generating prepared API file (please wait)…\n",
      "HINTS: file -> 檔案\n",
      "\n",
      "=== [PROMPT 21/2711] (translation) ===\n",
      "SOURCE: Add Model to Toolbox…\n",
      "HINTS: toolbox -> 工具箱\n",
      "\n",
      "=== [PROMPT 22/2711] (translation) ===\n",
      "SOURCE: Tools\n",
      "HINTS: \n",
      "\n",
      "=== [PROMPT 23/2711] (translation) ===\n",
      "SOURCE: Open Model\n",
      "HINTS: \n",
      "\n",
      "=== [PROMPT 24/2711] (translation) ===\n",
      "SOURCE: Processing models (*.model3 *.MODEL3)\n",
      "HINTS: processing -> 處理\n",
      "\n",
      "=== [PROMPT 25/2711] (translation) ===\n",
      "SOURCE: The selected file does not contain a valid model\n",
      "HINTS: file -> 檔案\n",
      "\n",
      "=== [PROMPT 26/2711] (translation) ===\n",
      "SOURCE: Model with the same name already exists\n",
      "HINTS: \n",
      "\n",
      "=== [PROMPT 27/2711] (translation) ===\n",
      "SOURCE: There is already a model file with the same name. Overwrite?\n",
      "HINTS: file -> 檔案\n",
      "\n",
      "=== [PROMPT 28/2711] (translation) ===\n",
      "SOURCE: Add Script to Toolbox…\n",
      "HINTS: script -> 腳本 | toolbox -> 工具箱\n",
      "\n",
      "=== [PROMPT 29/2711] (translation) ===\n",
      "SOURCE: Tools\n",
      "HINTS: \n",
      "\n",
      "=== [PROMPT 30/2711] (translation) ===\n",
      "SOURCE: Add script(s)\n",
      "HINTS: script -> 腳本\n",
      "\n",
      "=== [PROMPT 31/2711] (translation) ===\n",
      "SOURCE: Processing scripts (*.py *.PY)\n",
      "HINTS: processing -> 處理\n",
      "\n",
      "=== [PROMPT 32/2711] (translation) ===\n",
      "SOURCE: Could not copy script '{}'\\n{}\n",
      "HINTS: script -> 腳本\n",
      "API Request / Response -> 要求 / 回應\n",
      "Request -> 要求\n",
      "Response -> 回應\n",
      "Generating prepared API file (please wait)… -> 生成準備好的 檔案（請稍候）…\n",
      "Add Model to Toolbox… -> 將模型加入工具箱…\n",
      "Tools -> 工具\n",
      "Open Model -> 開啟模型\n",
      "Processing models (*.model3 *.MODEL3) -> 處理模型 ( )*.model3*.MODEL3\n",
      "The selected file does not contain a valid model -> 選取的檔案不包含有效的模型\n",
      "Model with the same name already exists -> 同名模型已存在\n",
      "There is already a model file with the same name. Overwrite? -> 已經有同名模型檔案。要覆蓋嗎？\n",
      "Add Script to Toolbox… -> 將腳本加入工具箱…\n",
      "Tools -> 工具\n",
      "Add script(s) -> 新增腳本()\n",
      "Processing scripts (*.py *.PY) -> 處理腳本 ( )*.py*.PY\n",
      "Could not copy script '{}'\\n{} -> 無法複製腳本 '{}' {} 無 法 複製 腳 本 '{}'\n",
      "Batch translating:   1%|          | 2/170 [00:59<1:20:24, 28.72s/it]\n",
      "=== [PROMPT 33/2711] (translation) ===\n",
      "SOURCE: Create New Script from Template…\n",
      "HINTS: script -> 腳本 | template -> 樣板\n",
      "\n",
      "=== [PROMPT 34/2711] (translation) ===\n",
      "SOURCE: Tools\n",
      "HINTS: \n",
      "\n",
      "=== [PROMPT 35/2711] (translation) ===\n",
      "SOURCE: Polygon intersection\n",
      "HINTS: polygon -> 多邊形\n",
      "\n",
      "=== [PROMPT 36/2711] (translation) ===\n",
      "SOURCE: Vectorize raster layer\n",
      "HINTS: raster layer -> 網格圖層 | raster -> 網格\n",
      "\n",
      "=== [PROMPT 37/2711] (translation) ===\n",
      "SOURCE: Interpolate (Inverse distance weighting)\n",
      "HINTS: \n",
      "\n",
      "=== [PROMPT 38/2711] (translation) ===\n",
      "SOURCE: RGB to PCT\n",
      "HINTS: \n",
      "\n",
      "=== [PROMPT 39/2711] (translation) ===\n",
      "SOURCE: Rasterize vector layer\n",
      "HINTS: vector -> 向量\n",
      "\n",
      "=== [PROMPT 40/2711] (translation) ===\n",
      "SOURCE: Polygon identity\n",
      "HINTS: polygon -> 多邊形\n",
      "\n",
      "=== [PROMPT 41/2711] (translation) ===\n",
      "SOURCE: Polygon dissolve (all polygons)\n",
      "HINTS: polygon -> 多邊形\n",
      "\n",
      "=== [PROMPT 42/2711] (translation) ===\n",
      "SOURCE: Polygon union\n",
      "HINTS: polygon -> 多邊形 | union -> 聯集\n",
      "\n",
      "=== [PROMPT 43/2711] (translation) ===\n",
      "SOURCE: Interpolate (Natural neighbor)\n",
      "HINTS: \n",
      "\n",
      "=== [PROMPT 44/2711] (translation) ===\n",
      "SOURCE: Merge raster layers\n",
      "HINTS: raster -> 網格\n",
      "\n",
      "=== [PROMPT 45/2711] (translation) ===\n",
      "SOURCE: Remove small pixel clumps (nearest neighbour)\n",
      "HINTS: \n",
      "\n",
      "=== [PROMPT 46/2711] (translation) ===\n",
      "SOURCE: Interpolate (Nearest Neighbor)\n",
      "HINTS: \n",
      "\n",
      "=== [PROMPT 47/2711] (translation) ===\n",
      "SOURCE: Interpolate (Cubic spline)\n",
      "HINTS: \n",
      "\n",
      "=== [PROMPT 48/2711] (translation) ===\n",
      "SOURCE: Interpolate (Data metrics)\n",
      "HINTS: data -> 資料\n",
      "Create New Script from Template… -> 從樣板建立新腳本…\n",
      "Tools -> 工具\n",
      "Polygon intersection -> 多邊形交集\n",
      "Vectorize raster layer -> 網格圖層向量化\n",
      "Interpolate (Inverse distance weighting) -> 內插法（反距離加權）\n",
      "RGB to PCT -> 至\n",
      "Rasterize vector layer -> 向量化層繪製\n",
      "Polygon identity -> 多邊形身份\n",
      "Polygon dissolve (all polygons) -> 多邊形溶解（所有多邊形）\n",
      "Polygon union -> 多邊形聯集\n",
      "Interpolate (Natural neighbor) -> 內插 (自然鄰居)\n",
      "Merge raster layers -> 合併網格層\n",
      "Remove small pixel clumps (nearest neighbour) -> 移除小像素塊（最近鄰居）\n",
      "Interpolate (Nearest Neighbor) -> 內插 (最近鄰居)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "import sys\n",
    "import glob\n",
    "import html\n",
    "import logging\n",
    "import warnings\n",
    "from typing import Optional, List, Tuple, Dict, Any\n",
    "\n",
    "import pandas as pd\n",
    "from lxml import etree\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# ================== 避免輸出一堆訊息 ==================\n",
    "os.environ.setdefault(\"HF_HUB_DISABLE_PROGRESS_BARS\", \"1\")\n",
    "os.environ.setdefault(\"HF_HUB_DISABLE_TELEMETRY\", \"1\")\n",
    "os.environ.setdefault(\"TRANSFORMERS_VERBOSITY\", \"error\")\n",
    "os.environ.setdefault(\"PYTHONWARNINGS\", \"ignore\")\n",
    "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
    "os.environ.setdefault(\"BITSANDBYTES_NOWELCOME\", \"1\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "for name in [\"transformers\", \"huggingface_hub\", \"urllib3\", \"accelerate\", \"bitsandbytes\", \"torch\"]:\n",
    "    logging.getLogger(name).setLevel(logging.ERROR)\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "try:\n",
    "    from transformers.utils import logging as hf_logging\n",
    "    hf_logging.set_verbosity_error()\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    from huggingface_hub.utils import logging as hub_logging\n",
    "    hub_logging.set_verbosity(hub_logging.ERROR)\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    from tokenizers import logging as tk_logging\n",
    "    tk_logging.set_verbosity_error()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ================== 全域設定 ==================\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "os.environ[\"BITSANDBYTES_NOWELCOME\"] = \"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"2\")\n",
    "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True,max_split_size_mb:128\")\n",
    "\n",
    "# ---- 參數設定 ----\n",
    "NEWTOK_RATIO     = float(os.getenv(\"NEWTOK_RATIO\", \"1.4\"))\n",
    "\n",
    "DEBUG_SHOW_CHAT  = os.getenv(\"DEBUG_SHOW_CHAT\", \"0\").strip().lower() in (\"1\",\"true\",\"yes\",\"y\",\"on\")\n",
    "DEBUG_PROMPTS    = os.getenv(\"DEBUG_PROMPTS\", \"1\").strip().lower() in (\"1\",\"true\",\"yes\",\"y\",\"on\")\n",
    "PRINT_RAW_ZH     = os.getenv(\"PRINT_RAW_ZH\", \"0\").strip().lower() in (\"1\",\"true\",\"yes\",\"y\",\"on\")\n",
    "PROMPT_MAX_CHARS = int(os.getenv(\"PROMPT_MAX_CHARS\", \"0\"))  # 0=不截斷\n",
    "\n",
    "for k in (\"HF_TOKEN\", \"HUGGINGFACEHUB_API_TOKEN\", \"HUGGING_FACE_HUB_TOKEN\"):\n",
    "    if API_KEY:\n",
    "        os.environ[k] = API_KEY\n",
    "\n",
    "# ================== 小工具 ==================\n",
    "def dprint(msg: str = \"\"):\n",
    "    if DEBUG_PROMPTS:\n",
    "        print(msg, file=sys.stderr, flush=True)\n",
    "\n",
    "def _truncate(s: str) -> str:\n",
    "    if PROMPT_MAX_CHARS and len(s) > PROMPT_MAX_CHARS:\n",
    "        return s[:PROMPT_MAX_CHARS] + \"\\n...[truncated]...\"\n",
    "    return s\n",
    "\n",
    "def to_one_line(s: str) -> str:\n",
    "    return (s or \"\").replace(\"\\r\", \"\").replace(\"\\n\", \"\\\\n\").replace(\"\\t\", \"\\\\t\")\n",
    "\n",
    "try:\n",
    "    from transformers import BitsAndBytesConfig\n",
    "    _HAS_BNB = True\n",
    "except Exception:\n",
    "    _HAS_BNB = False\n",
    "\n",
    "def _pick_dtype() -> torch.dtype:\n",
    "    if not torch.cuda.is_available():\n",
    "        return torch.float32\n",
    "    cap_major = torch.cuda.get_device_capability(0)[0]\n",
    "    return torch.bfloat16 if cap_major >= 8 else torch.float16\n",
    "\n",
    "# ================== LLM 載入 ==================\n",
    "def load_llm(model_name: str, fallback_name: str):\n",
    "    def _load(name: str, quant: Optional[str]):\n",
    "        hf_tok = os.environ.get(\"HF_TOKEN\", None)\n",
    "        try:\n",
    "            tok = AutoTokenizer.from_pretrained(name, use_fast=True, token=hf_tok)\n",
    "        except TypeError:\n",
    "            tok = AutoTokenizer.from_pretrained(name, use_fast=True, use_auth_token=hf_tok)\n",
    "        tok.padding_side = \"left\"\n",
    "        if tok.pad_token_id is None:\n",
    "            tok.pad_token_id = tok.eos_token_id\n",
    "\n",
    "        qcfg = None\n",
    "        if _HAS_BNB and torch.cuda.is_available() and quant:\n",
    "            if quant == \"4bit\":\n",
    "                qcfg = BitsAndBytesConfig(\n",
    "                    load_in_4bit=True,\n",
    "                    bnb_4bit_compute_dtype=_pick_dtype(),\n",
    "                    bnb_4bit_quant_type=\"nf4\",\n",
    "                    bnb_4bit_use_double_quant=True,\n",
    "                )\n",
    "            elif quant == \"8bit\":\n",
    "                qcfg = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "        try:\n",
    "            mdl = AutoModelForCausalLM.from_pretrained(\n",
    "                name,\n",
    "                torch_dtype=_pick_dtype(),\n",
    "                device_map=(\"auto\" if torch.cuda.is_available() else None),\n",
    "                low_cpu_mem_usage=True,\n",
    "                quantization_config=qcfg,\n",
    "                attn_implementation=\"sdpa\",\n",
    "                token=hf_tok,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "        except TypeError:\n",
    "            mdl = AutoModelForCausalLM.from_pretrained(\n",
    "                name,\n",
    "                torch_dtype=_pick_dtype(),\n",
    "                device_map=(\"auto\" if torch.cuda.is_available() else None),\n",
    "                low_cpu_mem_usage=True,\n",
    "                quantization_config=qcfg,\n",
    "                attn_implementation=\"sdpa\",\n",
    "                use_auth_token=hf_tok,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "        mdl.eval()\n",
    "        if getattr(mdl.generation_config, \"pad_token_id\", None) is None:\n",
    "            mdl.generation_config.pad_token_id = tok.pad_token_id\n",
    "        return tok, mdl\n",
    "\n",
    "    last_err = None\n",
    "    for name, quant in [\n",
    "        (model_name, \"4bit\"), (model_name, \"8bit\"), (model_name, None),\n",
    "        (fallback_name, \"4bit\"), (fallback_name, \"8bit\"), (fallback_name, None)\n",
    "    ]:\n",
    "        try:\n",
    "            tok, mdl = _load(name, quant)\n",
    "            return tok, mdl, name\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            continue\n",
    "    raise RuntimeError(f\"載入模型失敗：{last_err}\")\n",
    "\n",
    "tokenizer, model, _ = load_llm(MODEL, FALLBACK_MODEL)\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU count:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Current GPU:\", torch.cuda.get_device_name(0))\n",
    "print(\"Model repo:\", getattr(model.config, \"_name_or_path\", None))\n",
    "try:\n",
    "    print(\"Model dtype:\", getattr(next(iter(model.parameters())), \"dtype\", \"n/a\"))\n",
    "except StopIteration:\n",
    "    print(\"Model dtype: n/a\")\n",
    "print(\"Device map keys (sample):\", list(getattr(model, \"hf_device_map\", {\"(no map)\": \"...\"}))[:8])\n",
    "\n",
    "# ================== 英文檢測 & HTML 過濾 ==================\n",
    "_EN2 = re.compile(r\"[A-Za-z]\")\n",
    "def english_letter_count(s: Optional[str]) -> int:\n",
    "    if not s: return 0\n",
    "    return len(_EN2.findall(s))\n",
    "\n",
    "def clean_visible_text(s: str) -> str:\n",
    "    if s is None: return \"\"\n",
    "    s = html.unescape(s)\n",
    "    s = re.sub(r\"<[^>]+>\", \"\", s)\n",
    "    return s.strip()\n",
    "\n",
    "# ================== 詞庫（ODS） ==================\n",
    "def load_lookup_from_ods(folder: str = \"data\") -> pd.DataFrame:\n",
    "    paths = sorted(glob.glob(os.path.join(folder, \"*.ods\")))\n",
    "    if not paths:\n",
    "        print(f\"[提示] 詞庫資料夾 {folder} 內找不到 .ods 檔案，將不提供 glossary。\", file=sys.stderr)\n",
    "        return pd.DataFrame({\"en\": [], \"zh\": []})\n",
    "    rows = []\n",
    "    for p in paths:\n",
    "        try:\n",
    "            df = pd.read_excel(p, engine=\"odf\")\n",
    "            if \"英文名稱\" in df.columns and \"中文名稱\" in df.columns:\n",
    "                sub = df[[\"英文名稱\", \"中文名稱\"]].copy()\n",
    "                sub.columns = [\"en\", \"zh\"]\n",
    "                rows.append(sub)\n",
    "            else:\n",
    "                print(f\"[略過] {p} 缺少『英文名稱/中文名稱』欄位\", file=sys.stderr)\n",
    "        except Exception as e:\n",
    "            print(f\"[警告] 無法讀取 {p}: {e}\", file=sys.stderr)\n",
    "    if not rows:\n",
    "        print(\"[提示] 未取得任何有效的詞庫資料。\", file=sys.stderr)\n",
    "        return pd.DataFrame({\"en\": [], \"zh\": []})\n",
    "    out = pd.concat(rows, ignore_index=True)\n",
    "    out = out.dropna(subset=[\"en\", \"zh\"]).drop_duplicates().reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "# ================== LCS / Glossary ==================\n",
    "def soft_norm(s: str) -> str:\n",
    "    return re.sub(r'[\\s/_\\-.:]+', ' ', s.lower()).strip()\n",
    "\n",
    "_GLOSSARY_FILTER_PAT = re.compile(\n",
    "    r'(</?[A-Za-z][^>]*>|&lt;/?[A-Za-z][^&]*?&gt;|%L\\d+|%\\d+|%n|\\{\\d+\\}|&(?:[A-Za-z]+|#\\d+|#x[0-9A-Fa-f]+);)',\n",
    "    flags=re.IGNORECASE\n",
    ")\n",
    "def _clean_for_glossary(text: str) -> str:\n",
    "    return _GLOSSARY_FILTER_PAT.sub(' ', text)\n",
    "\n",
    "class LCSMatcher:\n",
    "    _TOKEN_RE = re.compile(r\"[A-Za-z0-9]+(?:[\\/_\\.:\\-][A-Za-z0-9]+)*\")\n",
    "    def __init__(self, lookup_df: pd.DataFrame):\n",
    "        self.lookup = lookup_df.copy()\n",
    "        if not self.lookup.empty:\n",
    "            self.lookup[\"en_soft\"] = self.lookup[\"en\"].apply(soft_norm)\n",
    "            self.soft_index: Dict[str, Tuple[str, str]] = {}\n",
    "            for _, row in self.lookup.iterrows():\n",
    "                key = row[\"en_soft\"]\n",
    "                if key not in self.soft_index:\n",
    "                    self.soft_index[key] = (row[\"en\"], row[\"zh\"])\n",
    "            self.max_soft_len = max((len(x.split()) for x in self.lookup[\"en_soft\"]), default=1)\n",
    "        else:\n",
    "            self.soft_index = {}\n",
    "            self.max_soft_len = 1\n",
    "\n",
    "    def build_glossary_sentence_first(self, text: str, *, limit: int = 8) -> Dict[str, str]:\n",
    "        if not self.soft_index: return {}\n",
    "        text_clean = _clean_for_glossary(text)\n",
    "        toks = self._TOKEN_RE.findall(text_clean.lower())\n",
    "        glossary: Dict[str, str] = {}\n",
    "        for w in range(min(len(toks), self.max_soft_len), 0, -1):\n",
    "            if len(glossary) >= limit: break\n",
    "            for i in range(0, len(toks) - w + 1):\n",
    "                key = soft_norm(\" \".join(toks[i:i+w]))\n",
    "                if key in self.soft_index and self.soft_index[key][0] not in glossary:\n",
    "                    en, zh = self.soft_index[key]\n",
    "                    if re.search(r\"[；;、/]|(?:^|[^一-龥])或([^一-龥]|$)\", str(zh)):\n",
    "                        continue\n",
    "                    glossary[en] = zh\n",
    "                    if len(glossary) >= limit: break\n",
    "        return glossary\n",
    "\n",
    "# ================== Prompt ==================\n",
    "SYS_PROMPT = (\n",
    "    \"You are translating software UI strings into zh-Hant (Taiwan).\\n\"\n",
    "    \"Translate ONLY the human-readable English text. Do NOT change, translate, add, \"\n",
    "    \"remove, re-order, or add spaces around ANY of the following:\\n\"\n",
    "    \"• HTML/XML tags and their attributes (e.g., <b>, </p>, <a href=\\\"...\\\">\\n\"\n",
    "    \"• HTML/XML entities: &NAME; (e.g., &amp;, &lt;, &gt;, &nbsp;), numeric (&#123;), hex (&#x1A;)\\n\"\n",
    "    \"• Qt/printf-style placeholders & format specifiers: %n, %1, %2, %L1, {0}, {1}, {2}, \"\n",
    "    \"%#, %s, %d, %f, %.2f, %.*f, %%\\n\"\n",
    "    \"Never invent or output artificial placeholders like @@0@@, @@1@@, or empty braces {}.\\n\"\n",
    "    \"Keep ALL of those tokens EXACTLY as in SOURCE (same spelling, case, order, and spacing).\\n\"\n",
    "    \"If a string mixes text and tags/placeholders, translate only the plain English words and \"\n",
    "    \"leave tags/placeholders untouched and in the same positions.\\n\"\n",
    "    \"Do NOT echo the source or the hints and do NOT add explanations.\\n\"\n",
    "    \"Reply in this exact format:\\n\"\n",
    "    \"<zh>{translation}</zh>\\n\"\n",
    "    \"If the source is empty, reply <zh></zh>.\\n\"\n",
    ")\n",
    "\n",
    "STRICT_SYS_PROMPT = SYS_PROMPT + (\n",
    "    \"\\nIMPORTANT:\\n\"\n",
    "    \"- The output MUST contain Chinese characters (CJK). If you leave any English letters \"\n",
    "    \"outside the allowed tokens/tags/placeholders, that is a failure.\\n\"\n",
    "    \"- For very short UI labels, ALWAYS translate (e.g., Error→錯誤, Request→請求, Response→回應, Tools→工具).\\n\"\n",
    ")\n",
    "\n",
    "def apply_chat_template_strict(tokenizer, user_prompt: str) -> str:\n",
    "    try:\n",
    "        return tokenizer.apply_chat_template(\n",
    "            [{\"role\": \"system\", \"content\": STRICT_SYS_PROMPT},\n",
    "             {\"role\": \"user\",   \"content\": user_prompt}],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "    except Exception:\n",
    "        return \"SYSTEM:\\n\" + STRICT_SYS_PROMPT.strip() + \"\\n\\nUSER:\\n\" + user_prompt + \"\\n\\nASSISTANT:\"\n",
    "\n",
    "def build_user_prompt(source_text: str, hints: Dict[str, str]) -> str:\n",
    "    lines = []\n",
    "    lines.append(\"SOURCE:\")\n",
    "    lines.append(source_text if source_text is not None else \"\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"HINTS (do not copy, just reference):\")\n",
    "    if hints:\n",
    "        for en, zh in list(hints.items()):\n",
    "            lines.append(f\"- {en} -> {zh}\")\n",
    "    else:\n",
    "        lines.append(\"(none)\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"Reply ONLY with one line: <zh>{translation}</zh>\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def apply_chat_template(tokenizer, user_prompt: str) -> str:\n",
    "    try:\n",
    "        return tokenizer.apply_chat_template(\n",
    "            [{\"role\": \"system\", \"content\": SYS_PROMPT},\n",
    "             {\"role\": \"user\",   \"content\": user_prompt}],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "    except Exception:\n",
    "        return \"SYSTEM:\\n\" + SYS_PROMPT.strip() + \"\\n\\nUSER:\\n\" + user_prompt + \"\\n\\nASSISTANT:\"\n",
    "\n",
    "# ================== 後處理 ==================\n",
    "_ZH_TAG_RE = re.compile(r\"<zh>(.*?)</zh>\", flags=re.S)\n",
    "META_LINE_PAT = re.compile(r\"^(?:- |\\u2022)?\\s*(?:原文|譯文|翻譯|Translation|Original)\\s*[:：]\", flags=re.IGNORECASE)\n",
    "\n",
    "def drop_meta_lines(s: str) -> str:\n",
    "    kept = []\n",
    "    for ln in s.splitlines():\n",
    "        if META_LINE_PAT.search(ln): continue\n",
    "        kept.append(ln)\n",
    "    return \"\\n\".join(kept).strip()\n",
    "\n",
    "def strip_label_prefixes(src: str, out_s: str) -> str:\n",
    "    if english_letter_count(src) >= 2 or src.strip().startswith(\"<\") or src.strip().startswith(\"&lt;\"):\n",
    "        out_s = re.sub(r\"^(?:原文|譯文|翻譯|Translation|Original|請求|回應|說明)\\s*[:：]\\s*\", \"\", out_s)\n",
    "    return out_s\n",
    "\n",
    "_ENC_TAG_RE       = re.compile(r\"&lt;/?[A-Za-z][^&]*?&gt;\")\n",
    "_TAG_RE           = re.compile(r\"</?[^>]+?>\")\n",
    "_ENTITY_RE2       = re.compile(r\"&(?:[A-Za-z]+|#[0-9]+|#x[0-9A-Fa-f]+);\")\n",
    "_PLACEHOLDER_RE_1 = re.compile(r\"%(?:L\\d+|\\d+|n)\")\n",
    "_PLACEHOLDER_RE_2 = re.compile(r\"\\{\\d+\\}\")\n",
    "_GLOB_RE          = re.compile(r\"(?<!\\w)(?:\\*|\\?)[A-Za-z0-9._-]+\")\n",
    "\n",
    "def strip_added_spans_not_in_src(src: str, out_s: str) -> str:\n",
    "    allowed = set()\n",
    "    for pat in (_ENC_TAG_RE, _TAG_RE, _ENTITY_RE2, _PLACEHOLDER_RE_1, _PLACEHOLDER_RE_2, _GLOB_RE):\n",
    "        for m in pat.finditer(src):\n",
    "            allowed.add(m.group(0))\n",
    "    def _filter(pat: re.Pattern, text: str) -> str:\n",
    "        return pat.sub(lambda m: m.group(0) if m.group(0) in allowed else \"\", text)\n",
    "    for pat in (_ENC_TAG_RE, _TAG_RE, _ENTITY_RE2, _PLACEHOLDER_RE_1, _PLACEHOLDER_RE_2, _GLOB_RE):\n",
    "        out_s = _filter(pat, out_s)\n",
    "    return out_s\n",
    "\n",
    "def trim_quotes(s: str) -> str:\n",
    "    return re.sub(r'^(?:[“”\"「」])+|(?:[“”\"「」])+$', \"\", s).strip()\n",
    "\n",
    "def enforce_zh_only_preserve_tags(text: str) -> str:\n",
    "    if not text:\n",
    "        return text\n",
    "    keep_spans = {}\n",
    "    def _ins(span: str) -> str:\n",
    "        key = f\"@@K{len(keep_spans)}@@\"\n",
    "        keep_spans[key] = span\n",
    "        return key\n",
    "    tmp = text\n",
    "    for pat in (_ENC_TAG_RE, _TAG_RE, _ENTITY_RE2, _PLACEHOLDER_RE_1, _PLACEHOLDER_RE_2):\n",
    "        tmp = pat.sub(lambda m: _ins(m.group(0)), tmp)\n",
    "    tmp = re.sub(r\"[A-Za-z]+\", \"\", tmp)\n",
    "    tmp = re.sub(r\"[^\\s\\u4E00-\\u9FFF\\u3400-\\u4DBF\\uF900-\\uFAFF\\u3000-\\u303F\\uFF00-\\uFFEF\\u2000-\\u206F0-9\\uFF10-\\uFF19\\u0021-\\u002F\\u003A-\\u0040\\u005B-\\u0060\\u007B-\\u007E]\", \"\", tmp)\n",
    "    tmp = re.sub(r\"\\s+\", \" \", tmp).strip()\n",
    "    for k, v in keep_spans.items():\n",
    "        tmp = tmp.replace(k, v)\n",
    "    return tmp\n",
    "\n",
    "# === Token 修復 ===\n",
    "LOCKED_RE_STR = r\"(?:&lt;/?[A-Za-z][^&]*?&gt;|</?[^>]+?>|&(?:[A-Za-z]+|#[0-9]+|#x[0-9A-Fa-f]+);|%(?:L\\d+|\\d+|n)|\\{\\d+\\}|(?<!\\w)(?:\\*|\\?)[A-Za-z0-9._-]+)\"\n",
    "LOCKED_RE = re.compile(LOCKED_RE_STR)\n",
    "ATAT_TOKEN_RE = re.compile(r\"@@\\d+@@\")\n",
    "\n",
    "def _collect_locked_tokens(src: str) -> list[str]:\n",
    "    return [m.group(0) for m in LOCKED_RE.finditer(src)]\n",
    "\n",
    "def _prefix_locked_concat(src: str) -> str:\n",
    "    out, pos = [], 0\n",
    "    while True:\n",
    "        m = LOCKED_RE.match(src, pos)\n",
    "        if not m: break\n",
    "        out.append(m.group(0))\n",
    "        pos = m.end()\n",
    "    return \"\".join(out)\n",
    "\n",
    "def _suffix_locked_concat(src: str) -> str:\n",
    "    out = []\n",
    "    pos = len(src)\n",
    "    # 尋找連續尾段 token\n",
    "    while True:\n",
    "        last = None\n",
    "        for mm in LOCKED_RE.finditer(src):\n",
    "            if mm.end() == pos:\n",
    "                last = mm\n",
    "        if not last: break\n",
    "        out.append(last.group(0))\n",
    "        pos = last.start()\n",
    "        if pos == 0 or not LOCKED_RE.match(src, pos - 1):\n",
    "            break\n",
    "    out.reverse()\n",
    "    return \"\".join(out)\n",
    "\n",
    "def repair_tokens_from_source(src: str, text: str) -> str:\n",
    "    if not text:\n",
    "        return text\n",
    "    tokens = _collect_locked_tokens(src)\n",
    "    idx = 0\n",
    "    def _repl(_m):\n",
    "        nonlocal idx\n",
    "        t = tokens[idx] if idx < len(tokens) else \"\"\n",
    "        idx += 1\n",
    "        return t\n",
    "    text = ATAT_TOKEN_RE.sub(_repl, text)\n",
    "    text = strip_added_spans_not_in_src(src, text)\n",
    "    pref = _prefix_locked_concat(src)\n",
    "    suf  = _suffix_locked_concat(src)\n",
    "    if pref and not text.startswith(pref):\n",
    "        text = pref + text\n",
    "    if suf and not text.endswith(suf):\n",
    "        text = text + suf\n",
    "    if idx < len(tokens):\n",
    "        text += \"\".join(tokens[idx:])\n",
    "    return text\n",
    "\n",
    "# === 若變成空或無中文 → 回退原文 ===\n",
    "_CJK_RE = re.compile(r\"[\\u4E00-\\u9FFF\\u3400-\\u4DBF\\uF900-\\uFAFF]\")\n",
    "def _looks_nonsense(src: str, s: str) -> bool:\n",
    "    v = clean_visible_text(s)\n",
    "    if v == \"{}\":\n",
    "        return True\n",
    "    if re.fullmatch(r\"[\\s\\{\\}\\[\\]\\(\\)《》〈〉「」『』、，。．…—\\-–~!@#$%^&*_|\\\\:;\\\"'`,.*?/]*\", v or \"\"):\n",
    "        return True\n",
    "    if re.fullmatch(r\"\\{\\d*\\}\", v or \"\") and (v not in src):\n",
    "        return True\n",
    "    if not _CJK_RE.search(s or \"\"):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def extract_zh_or_fallback(src: str, model_out: str) -> str:\n",
    "    if not model_out:\n",
    "        return src\n",
    "    m = _ZH_TAG_RE.search(model_out)\n",
    "    s = (m.group(1) if m else model_out).strip()\n",
    "    s = drop_meta_lines(s)\n",
    "    s = trim_quotes(s)\n",
    "    s = strip_label_prefixes(src, s)\n",
    "    s = strip_added_spans_not_in_src(src, s)\n",
    "    s = enforce_zh_only_preserve_tags(s)\n",
    "    s = ATAT_TOKEN_RE.sub(\"\", s)\n",
    "    if \"{}\" in s and \"{}\" not in src:\n",
    "        s = s.replace(\"{}\", \"\")\n",
    "    s = repair_tokens_from_source(src, s)\n",
    "    if _looks_nonsense(src, s):\n",
    "        s = src\n",
    "    return s\n",
    "\n",
    "# ================== 後備字典（在重試仍失敗時使用） ==================\n",
    "UI_FALLBACK_DICT = {\n",
    "    \"Error\": \"錯誤\",\n",
    "    \"Request\": \"請求\",\n",
    "    \"Response\": \"回應\",\n",
    "    \"API Request / Response\": \"API 請求 / 回應\",\n",
    "    \"Tools\": \"工具\",\n",
    "    \"Open Model\": \"開啟模型\",\n",
    "    \"Input parameters\": \"輸入參數\",\n",
    "    \"Outputs\": \"輸出\",\n",
    "    \"There is no active layer.\": \"沒有活動圖層。\",\n",
    "    \"Help author: {0}\": \"說明作者：{0}\",\n",
    "    \"Algorithm author: {0}\": \"演算法作者：{0}\",\n",
    "    \"Algorithm version: {0}\": \"演算法版本：{0}\",\n",
    "    \"Could not prepare selected algorithm.\": \"無法準備所選的演算法。\",\n",
    "    \"Error adding processed features back into the layer.\": \"將處理後的要素加回圖屠時發生錯誤。\",\n",
    "    \"Generating prepared API file (please wait)…\": \"正在產生預先準備的 API 檔案（請稍候）…\",\n",
    "    \"Add Model to Toolbox…\": \"新增模型至工具箱…\",\n",
    "    \"Add Script to Toolbox…\": \"新增腳本至工具箱…\",\n",
    "    \"Add script(s)\": \"新增腳本\",\n",
    "    \"Processing models (*.model3 *.MODEL3)\": \"處理模型 (*.model3 *.MODEL3)\",\n",
    "    \"Processing scripts (*.py *.PY)\": \"處理腳本 (*.py *.PY)\",\n",
    "    \"The selected file does not contain a valid model\": \"選取的檔案不包含有效的模型\",\n",
    "    \"Model with the same name already exists\": \"已有相同名稱的模型\",\n",
    "    \"There is already a model file with the same name. Overwrite?\": \"已有同名的模型檔案。是否覆寫？\",\n",
    "    \"Rasterize mesh dataset\": \"網格資料集光柵化\",\n",
    "}\n",
    "\n",
    "def dict_fallback_translate(src: str) -> Optional[str]:\n",
    "    key = re.sub(r\"\\s+\", \" \", (src or \"\").strip())\n",
    "    if key in UI_FALLBACK_DICT:\n",
    "        return repair_tokens_from_source(src, UI_FALLBACK_DICT[key])\n",
    "    return None\n",
    "\n",
    "# ================== 翻譯（batch ） ==================\n",
    "BAD_WORDS = [\"assistant\",\"user\",\"system\",\"以下\",\"翻譯\",\"說明\",\"根據\",\"提示\",\"僅供參考\"]\n",
    "def build_bad_words_ids(tokenizer) -> List[List[int]]:\n",
    "    bad_ids: List[List[int]] = []\n",
    "    for w in BAD_WORDS:\n",
    "        ids = tokenizer.encode(w, add_special_tokens=False)\n",
    "        if ids: bad_ids.append(ids)\n",
    "    return bad_ids\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate_texts(tokenizer, model, prompts: List[str]) -> List[str]:\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    bad_words_ids = build_bad_words_ids(tokenizer)\n",
    "\n",
    "    def _run(ps: List[str], max_new_tokens_cap: int):\n",
    "        # 以單一 MAX_TOKENS 負責「輸入截斷上限」\n",
    "        max_ctx = getattr(tokenizer, \"model_max_length\", MAX_TOKENS)\n",
    "        enc = tokenizer(\n",
    "            ps,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=min(MAX_TOKENS, max_ctx if max_ctx and max_ctx < 10**7 else MAX_TOKENS),\n",
    "        )\n",
    "        if torch.cuda.is_available():\n",
    "            enc = {k: v.cuda() for k, v in enc.items()}\n",
    "        input_ids = enc[\"input_ids\"]\n",
    "        nonpad_lens = (input_ids != tokenizer.pad_token_id).sum(dim=1).tolist()\n",
    "        wanted_per_row = [max(MIN_TOKENS, int(l * NEWTOK_RATIO)) for l in nonpad_lens]\n",
    "        # 以單一 MAX_TOKENS 作為「生成最大上限」\n",
    "        batch_max_new = min(max(wanted_per_row), max_new_tokens_cap)\n",
    "\n",
    "        gen = model.generate(\n",
    "            **enc,\n",
    "            max_new_tokens=batch_max_new,\n",
    "            do_sample=False,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            no_repeat_ngram_size=6,\n",
    "            bad_words_ids=bad_words_ids if bad_words_ids else None,\n",
    "            use_cache=True,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=False,\n",
    "        )\n",
    "        seq = gen.sequences\n",
    "        outs = []\n",
    "        for i in range(seq.size(0)):\n",
    "            in_len = int(nonpad_lens[i])\n",
    "            new_tokens = seq[i, in_len:]\n",
    "            s = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "            outs.append(s)\n",
    "        return outs\n",
    "\n",
    "    for cap in (MAX_TOKENS, max(64, MAX_TOKENS // 2), 32):\n",
    "        try:\n",
    "            return _run(prompts, cap)\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            torch.cuda.empty_cache(); gc.collect()\n",
    "\n",
    "    results = []\n",
    "    for pr in prompts:\n",
    "        ok = False\n",
    "        for cap in (max(64, MAX_TOKENS // 2), 32):\n",
    "            try:\n",
    "                results.extend(_run([pr], cap))\n",
    "                ok = True\n",
    "                break\n",
    "            except torch.cuda.OutOfMemoryError:\n",
    "                torch.cuda.empty_cache(); gc.collect()\n",
    "        if not ok:\n",
    "            results.append(\"\")\n",
    "    return results\n",
    "\n",
    "# ================== .ts 解析 ==================\n",
    "def clean_visible_text_for_ts(raw: str) -> str:\n",
    "    if raw is None: return \"\"\n",
    "    s = html.unescape(raw)\n",
    "    s = re.sub(r\"<[^>]+>\", \"\", s)\n",
    "    s = re.sub(r\"&lt;[^&]+?&gt;\", \"\", s)\n",
    "    return s.strip()\n",
    "\n",
    "def collect_ts_targets_use_source(root: etree._Element) -> Tuple[list, list, list]:\n",
    "    targets, sources, kinds = [], [], []\n",
    "\n",
    "    # numerus=\"yes\"：寫回 <numerusform>，來源用 <source>\n",
    "    for msg in root.xpath(\"//message[@numerus='yes']\"):\n",
    "        src_text = (msg.findtext(\"./source\") or \"\").strip()\n",
    "        trans = msg.find(\"./translation\")\n",
    "        if trans is None:\n",
    "            trans = etree.SubElement(msg, \"translation\")\n",
    "        nfs = trans.findall(\"./numerusform\")\n",
    "        if not nfs:\n",
    "            nf = etree.SubElement(trans, \"numerusform\")\n",
    "            nfs = [nf]\n",
    "        for nf in nfs:\n",
    "            targets.append(nf)\n",
    "            sources.append(src_text)\n",
    "            kinds.append(\"numerusform\")\n",
    "\n",
    "    # 一般狀況：寫回 <translation>，來源用 <source>\n",
    "    for msg in root.xpath(\"//message[not(@numerus='yes')]\"):\n",
    "        src = msg.find(\"./source\")\n",
    "        if src is None:\n",
    "            continue\n",
    "        source_text = (src.text or \"\").strip()\n",
    "        if not source_text:\n",
    "            continue\n",
    "        trans = msg.find(\"./translation\")\n",
    "        if trans is None:\n",
    "            trans = etree.SubElement(msg, \"translation\")\n",
    "        targets.append(trans)\n",
    "        sources.append(source_text)\n",
    "        kinds.append(\"translation\")\n",
    "\n",
    "    return targets, sources, kinds\n",
    "\n",
    "# ================== 批次翻譯 + 寫回 ==================\n",
    "def translate_ts_and_write(\n",
    "    ts_in: str,\n",
    "    ts_out: str,\n",
    "    ods_folder: str = \"data\",\n",
    "    *,\n",
    "    hint_terms_cap: int = 8,\n",
    "    batch_size: Optional[int] = None,\n",
    "):\n",
    "    tree = etree.parse(ts_in)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    lookup_df = load_lookup_from_ods(ods_folder)\n",
    "    matcher = LCSMatcher(lookup_df) if not lookup_df.empty else None\n",
    "\n",
    "    targets, sources, kinds = collect_ts_targets_use_source(root)\n",
    "    total = len(targets)\n",
    "    if total == 0:\n",
    "        print(\"沒有需要翻譯的項目。\")\n",
    "        return\n",
    "\n",
    "    print(f\"[TS] 待翻譯節點：{total}（translation 與 numerusform 皆以 <source> 為翻譯來源）\")\n",
    "\n",
    "    bs = max(1, batch_size or BATCH)\n",
    "\n",
    "    for i in tqdm(range(0, total, bs), desc=\"Batch translating\"):\n",
    "        seg_targets = targets[i:i + bs]\n",
    "        seg_sources = sources[i:i + bs]\n",
    "        seg_kinds   = kinds[i:i + bs]\n",
    "\n",
    "        prompts: List[str] = []\n",
    "        hints_list: List[Dict[str, str]] = []\n",
    "\n",
    "        for src_text in seg_sources:\n",
    "            vis = clean_visible_text(src_text)\n",
    "            hints = matcher.build_glossary_sentence_first(vis, limit=hint_terms_cap) if matcher is not None else {}\n",
    "            user_prompt = build_user_prompt(src_text, hints)\n",
    "            chat_prompt = apply_chat_template(tokenizer, user_prompt)\n",
    "            prompts.append(chat_prompt)\n",
    "            hints_list.append(hints)\n",
    "\n",
    "        # 除錯輸出\n",
    "        user_prompts_debug = [build_user_prompt(s, h) for s, h in zip(seg_sources, hints_list)]\n",
    "        for j, (src, up, cp, hints, kind) in enumerate(zip(seg_sources, user_prompts_debug, prompts, hints_list, seg_kinds)):\n",
    "            idx = i + j + 1\n",
    "            dprint(f\"\\n=== [PROMPT {idx}/{total}] ({kind}) ===\")\n",
    "            dprint(f\"SOURCE: {to_one_line(src)}\")\n",
    "            dprint(\"HINTS: \" + (\" | \".join([f\"{en} -> {zh}\" for en, zh in hints.items()]) if hints else \"\"))\n",
    "            if DEBUG_SHOW_CHAT:\n",
    "                dprint(\"RENDERED_CHAT_PROMPT:\")\n",
    "                dprint(_truncate(to_one_line(cp)))\n",
    "\n",
    "        # 推理\n",
    "        outs = generate_texts(tokenizer, model, prompts)\n",
    "\n",
    "        # 後處理 + 寫回\n",
    "        for node, src_text, out_text, kind, hints in zip(seg_targets, seg_sources, outs, seg_kinds, hints_list):\n",
    "            zh = extract_zh_or_fallback(src_text, out_text)\n",
    "\n",
    "            # 若仍是英文/原文，啟動重試\n",
    "            need_retry = (zh == src_text) or (not _CJK_RE.search(zh or \"\"))\n",
    "            if need_retry:\n",
    "                strict_user_prompt = build_user_prompt(src_text, hints) + (\n",
    "                    \"\\n\\nExamples:\\n\"\n",
    "                    \"- Error -> 錯誤\\n- Request -> 請求\\n- Response -> 回應\\n- Tools -> 工具\\n\"\n",
    "                )\n",
    "                strict_chat_prompt = apply_chat_template_strict(tokenizer, strict_user_prompt)\n",
    "                out2 = generate_texts(tokenizer, model, [strict_chat_prompt])[0]\n",
    "                zh2 = extract_zh_or_fallback(src_text, out2)\n",
    "                if (zh2 != src_text) and _CJK_RE.search(zh2 or \"\"):\n",
    "                    zh = zh2\n",
    "                else:\n",
    "                    # 嚴格重試仍失敗 → 試小字典保底\n",
    "                    zh3 = dict_fallback_translate(src_text)\n",
    "                    if zh3:\n",
    "                        zh = zh3\n",
    "\n",
    "            node.text = zh\n",
    "            if node.tag == \"translation\":\n",
    "                node.attrib.pop(\"type\", None)\n",
    "            elif node.tag == \"numerusform\":\n",
    "                parent = node.getparent()\n",
    "                if parent is not None and parent.tag == \"translation\":\n",
    "                    parent.attrib.pop(\"type\", None)\n",
    "\n",
    "            if not PRINT_RAW_ZH:\n",
    "                en_print = clean_visible_text_for_ts(src_text)\n",
    "                zh_print = clean_visible_text_for_ts(zh)\n",
    "            else:\n",
    "                en_print, zh_print = src_text, zh\n",
    "            dprint(f\"{to_one_line(en_print)} -> {to_one_line(zh_print)}\")\n",
    "\n",
    "            # 可選：提醒如果還遺失 token\n",
    "            miss_src_tokens = [t for t in _collect_locked_tokens(src_text) if t not in zh]\n",
    "            if miss_src_tokens:\n",
    "                dprint(f\"[WARN] missing tokens -> {' | '.join(miss_src_tokens)}\")\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    tree.write(ts_out, encoding=\"utf-8\", xml_declaration=True, pretty_print=True)\n",
    "    print(f\"✅ 已輸出翻譯檔案：{ts_out}\")\n",
    "\n",
    "# ================== 執行 ==================\n",
    "if __name__ == \"__main__\":\n",
    "    translate_ts_and_write(\n",
    "        ts_in=INPUT_FILENAME,\n",
    "        ts_out=OUTPUT_FILENAME,\n",
    "        ods_folder=ODS_DIR,\n",
    "        hint_terms_cap=8,\n",
    "        batch_size=BATCH,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55214920-780c-40b9-9477-83a84b64be05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
